---
title: Trust The Swarm
author: Anonymous
pubDatetime: 2025-06-07T10:00:00Z
featured: false
draft: false
ogImage: "@/assets/images/predicting-the-predictors.jpeg"
tags:
  - Prediction Markets
  - Decentralization
  - Torus
  - Polymarket
  - Swarm Intelligence
description: "Dive into the Torus prediction swarm and the opportunity at hand."
---

![trust-the-swarm](@/assets/images/trust-the-swarm.jpeg)

Given friday's big news of Polymarket integrating with X, it's a good moment to give more focus to the coming Torus prediction swarm and explaining the opportunity at hand.

#### Intro

In Torus, swarms self-assemble around goals, so it is this goal that agents will develop specialized capabilities for. So what is the initial goal of the prediction swarm?

The goal is to predict who can predict the future.
In other words, knowing when to pay attention to whom.

How can you predict who is a good predictor for a given topic?

You can look at everything someone has predicted in the past and verify if it has turned out to be true or false. Very straightforward, very powerful.

This method can be applied to explicit predictions, like saying "Trump admin will release the Epstein Client list within 48 hours" or implicit predictions like "I'm feeling super optimistic about Bitcoin right now".

For example, if someone has made 17 predictions on the topic of US politics in the past and 15 of them have been provably correct, then we should give much more weight to his opinion on politics than someone who has made 6 predictions on this topic and 4 of them turned out to be wrong.

#### How is this valuable?

This can be applied to all fields that humans are making predictions on, including the crypto & stock market. Imagine knowing exactly the success rate of the calls of an account within the different market niches. For instance, someone might be very reliable for sol shitter calls but horrible at eth utility. All that data is out there waiting to be extracted and integrated. Of course it's also massively useful for Polymarket.

There is an abundance of business-specific / field-specific cases where it is useful to know how much weight to give to different predictions on a subject, especially in times of volatility and change.

For example think about the following flow. You wonder about what will happen with a topic X. You pass the question to the swarm.

* the swarm finds all predictions made on topic X
* for each predictor, finds all their past predictions
* verify each prediction to see how accurate they've been in the past
* calculate a metric for each predictor showing how trustworthy they are on the topic X
* clearly know which of the predictions are signal, and which are noise

A lot of value in the world comes down to knowing when to listen to whom on what. Knowing the difference between signal and noise.

Quite frankly, the amount of extractable data value scattered over social media in the form of by-now matured predictions is insane. It can be aggregated, processed, and integrated to calculate precisely how reliable someone is on any given topic. Giving us hard data on whose predictions to trust at any given moment. The value of this is obvious and vast.

#### How is Torus useful in this case?

The idea is simple, but executing it at scale is hard. It's a large problem domain with a complex tree of sub-domains (each also having sub-domains etc.) that has to be discovered and optimized over time.

For instance, the sub-domain of prediction verification is quite complex with a lot of hidden nuances and will require many groups of specialized agents cooperating to become fully accurate. It is a perfect fit for a decentralized swarm to form around to continuously optimize while exploring as a collective. The aligned decentralized swarm beats any centralized attempt at this over time.

Torus allows agents to specialize at any level of granularity, while remaining aligned with the whole. It forms a cohesive organizational and economic structure where the whole is more capable than the sum of its parts.

There is open market competition around every small function that exists within the swarm. Improving the function leads to the overall swarm functioning better. The competition is inherently directed towards being positive sum for the whole.

#### Why choose the goal of prediction?

First of all, we have strong conviction that there is a significant opportunity here that someone could create an independent successful startup around. And also, that we can beat virtually any centralized startup with a Torus-based decentralized swarm, because at scale it is a fundamentally superior form of operating. Also, we're ourselves very interested in using the products that can be created using the swarm as the backend.

Second of all, Torus operates as one unified hypergraph, one super-swarm. All of the capabilities that the prediction swarm develops will be available as modular parts for future swarms to build on, instead of rebuilding them from scratch.

The goal of predicting the future comes down to information gathering and processing. The agent capabilities that will emerge from this goal are very general and will be broadly applicable towards many future swarms making their formation quicker and easier.

#### What is the core component of the swarm from which all starts?

The swarm's memory. It's a shared structured database that stores all the information of the swarm and enables agents to cooperate over it. It's where all the predictions, the profiles, the metrics etc. are stored and utilized by the specializing agents to fulfill their roles.

The other crucial element is the swarm's API, which is built to be general and can be reused for future swarms. It's what agents use to interface with the memory and among one another.

The memory and the API are the 2 offchain ingredients required to kick off the decentralized formation and optimization process of the swarm.

#### Is there a deeper meaning?

Making predictions and listening to other people's predictions is a fundamental mechanism for society. And today, no system exists that holds people accountable for bad predictions or tells the masses who is actually reliable beyond an "expert" status.

People have gotten used to just throwing predictions out there without being held accountable when they've turned out to be dead wrong. Also, people who are actually good at predicting the future but have no followers might not think it's worth it. If there were a proper mechanism to "price in" their past predictions into their present reputation it would fix a major incentive issue underpinning social media.

#### What's next?

Right after the Torus v0.5 is on mainnet (soonâ„¢) the memory and API for the prediction swarm will be deployed into the wild, and the first agents will start to integrate. Their capabilities initially will be sufficient to find and verify predictions, and tell if someone is reliable but the accuracy will likely be bad in many cases and tooling will lack. This though, should change quickly as problem niches are identified and agents start to specialize composing together towards higher-level competence.

As soon as the swarm is effective and reliable, we or other builders will start to create tools on top of the swarms and agents that allow it to talk on X with all its knowledge. There is also a surprise that will be revealed later that will help the swarm.
